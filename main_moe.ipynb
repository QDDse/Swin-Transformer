{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4babdd11-0989-47a9-ae92-70a9b23f19cf",
   "metadata": {},
   "source": [
    "# This is a notebook based on the swin-moe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d761ec0c-080f-45b2-834b-716e38e2a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9850331a-41a7-4d6b-abf6-d3b86846ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n",
      "To use FusedLAMB or FusedAdam, please install apex.\n"
     ]
    }
   ],
   "source": [
    "from tutel import system\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.utils import accuracy, AverageMeter\n",
    "\n",
    "from config import get_config\n",
    "from models import build_model\n",
    "from data import build_loader\n",
    "from lr_scheduler import build_scheduler\n",
    "from optimizer import build_optimizer\n",
    "from logger import create_logger\n",
    "from utils import NativeScalerWithGradNormCount, reduce_tensor\n",
    "from utils_moe import load_checkpoint, load_pretrained, save_checkpoint, auto_resume_helper, hook_scale_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83163d94-5cdb-45d0-833a-5b3e3b279841",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tutel import moe as tutel_moe\n",
    "except:\n",
    "    tutel_moe = None\n",
    "    print(\"Tutel has not been installed. To use Swin-MoE, please install Tutel; otherwise, just ignore this.\")\n",
    "    assert torch.__version__ >= '1.8.0', \"DDP-based MoE requires Pytorch >= 1.8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96aa527-e124-4043-a864-b356276542cb",
   "metadata": {},
   "source": [
    "## Gshard-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c468942-d181-4764-8e82-afd36135fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "def _one_hot_with_dtype(data, num_classes, dtype, hot_value=1):\n",
    "    result = torch.zeros([data.size(0), num_classes], device=data.device, dtype=dtype)\n",
    "    result.scatter_(1, data.unsqueeze(-1), hot_value)\n",
    "    return result\n",
    "\n",
    "def gshard_loss(scores_w_noise, top_ids):\n",
    "    num_samples, num_global_experts = int(scores_w_noise.size(0)), int(scores_w_noise.size(1))\n",
    "    mask = _one_hot_with_dtype(top_ids[:, 0], num_global_experts, dtype=scores_w_noise.dtype,\n",
    "        hot_value=num_global_experts / num_samples)\n",
    "    me = torch.sum(scores_w_noise, dim=0)\n",
    "    ce = torch.sum(mask, dim=0)\n",
    "    l_aux = torch.sum(me * ce) / num_samples\n",
    "    return l_aux\n",
    "\n",
    "def load_importance_loss(scores_wo_noise, topk_logits, num_global_experts, gate_noise):\n",
    "    def load_loss(scores_wo_noise, topk_logits, num_global_experts, gate_noise):\n",
    "        assert gate_noise > 0, \"`gate_noise` must be > 0 for normalization in load_importance_loss().\"\n",
    "        normal = Normal(\n",
    "            torch.tensor([0.0], device=scores_wo_noise.device),\n",
    "            torch.tensor([gate_noise / num_global_experts], device=scores_wo_noise.device),\n",
    "        )\n",
    "        threshold = topk_logits[:, -1].view(-1, 1).float()\n",
    "        diff = scores_wo_noise.float() - threshold.float()\n",
    "        prob = normal.cdf(diff)\n",
    "        Load = prob.sum(0)\n",
    "        l_load = Load.float().var() / (Load.float().mean() ** 2 + 1e-10)\n",
    "        return l_load\n",
    "\n",
    "    def importance_loss(scores_wo_noise):\n",
    "        Impi = scores_wo_noise.float().sum(0)\n",
    "        l_imp = Impi.float().var() / (Impi.float().mean() ** 2 + 1e-10)\n",
    "\n",
    "        return l_imp\n",
    "\n",
    "    l_imp = importance_loss(scores_wo_noise)\n",
    "    l_load = load_loss(scores_wo_noise, topk_logits, num_global_experts, gate_noise)\n",
    "    return (l_imp + l_load) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd424f-6904-4146-a20c-1f42af70c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '8888'\n",
    "world_size = 8\n",
    "rank = 0\n",
    "\n",
    "dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87c3311-b8ac-474f-9823-9fc77aa1fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lr_scheduler import build_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd44194-928d-4a9a-9861-e196de1ab6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
